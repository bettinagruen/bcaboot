% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bcajack.R
\name{bcajack}
\alias{bcajack}
\title{Nonparametric bias-corrected and accelerated bootstrap
    confidence limits}
\usage{
bcajack(x, B, func, ..., m = nrow(x), mr = 5, K = 2, J = 10,
  alpha = c(0.025, 0.05, 0.1, 0.16, 0.5, 0.84, 0.9, 0.95, 0.975),
  verbose = TRUE, sw = 0)
}
\arguments{
\item{x}{an nxp data matrix, rows are observed p-vectors, assumed
to be independently sampled from target population. [if p=1
then x can be a vector.]}

\item{B}{number of bootstrap replications. 'B' can also be a vector
of B bootstrap replications of the estimated parameter of
interest, computed separately.}

\item{func}{function \eqn{\hat{\theta}}=func(x) computing estimate of the
parameter of interest; func(x) should return a real value for
any n' x p matrix x', n' not necessarily equal to n.}

\item{...}{additional arguments for func.}

\item{m}{integer m <= n; collects the n rows of x into m groups to
speed up the jackknife calculations for estimating the
acceleration value 'a'; typically m=20 or 40; does not have to
exactly divide n.}

\item{mr}{if m < n then mr repetions of the randomly grouped
jackknife calculations are averaged.}

\item{K}{If K > 0, bcajack also returns estimates of 'internal
standard error', that is, of the variability due to stopping at
B bootstrap replications rather than going on to
infinity. These are obtained from a second type of jackknifing,
taking an average of K separate jackknife estimates, each
randomly splitting the B bootstrap replications into J groups.}

\item{J}{the number of groups into which the bootstrap replications are split}

\item{alpha}{percentiles desired for the bca confidence limits.}

\item{verbose}{logical for verbose messages}

\item{sw}{switch that controls output, eg sw=5 returns the B
bootstrap replications as well as the bca output.}

\item{rou}{rounding parameter for the output.}
}
\value{
a named list of several items:

\describe{
\item{lims}{first column shows the estimated bca confidence limits
at the requested alpha percentiles. These can be compared with the
standard limits \code{thetahat+sdboot*z[alpha]}, third column. The second
column "jacksd" gives the internal standard errors for the bca
limits, quite small in the example. Column 4, "pct", gives the
percentiles of the ordered B bootstrap replications corresponding
to the bca limits, eg the 897th largest replication equalling the
.975 bca limit .557.}
\item{stats}{ top line of stats shows 5 estimates: thet is func(x),
original point estimate of the parameter of interest; sdboot is its
bootstrap estimate of standard error; z0 is the bca bias correction
value, in this case quite negative; a is the "acceleration", a
component of the bca limits (nearly zero here); sdjack is the
jackknife estimate of standard error for thet. Bottom line gives
the internal standard errors for the five quantities above. This is
substantial for z0 above.}
\item{B.mean}{bootstrap sample size B, and the mean of the B
bootstrap replications theathat*.}
}
}
\description{
\code{bcajack} computes nonparametric confidence
    intervals for bootstrap estimates. For reproducibility, save or
    set the random number state before calling this routine.
}
\details{
Bootstrap confidence intervals depend on three elements:
\itemize{
\item the cdf of the B bootstrap replications \code{t[i]*, i=1...B}
\item the bias-correction number \code{z0=qnorm(#{t[i]* < t0}/B )}
where \code{t0=func(x)} is the original estimate
\item the acceleration number \eqn{a} that measures the rate of
change in \code{sd{t0}} as x, the data changes.
}

The first two of these depend only on the bootstrap distribution,
and not how it is generated: parametrically or
non-parametrically. Program bcajack can be used in a hybrid fashion
in which the vector \code{tt} of B bootstrap replications is first
generated from a parametric model.

So, in the diabetes example below, we might first draw bootstrap
samples \eqn{y^* \sim N(X\hat{\beta}, \hat{\sigma}^2I)} where
\eqn{\hat{beta}} and \eqn{\hat{sigma}} were obtained from
\code{lm(y~X)}; each \eqn{y^*} would then provide a bootstrap
replication \code{t* = rfun(cbind(X,y*))}.  Then we could get bca
intervals from \code{bcajack(Xy, tt, rfun ....)} with \code{tt},
the vector of B \code{t*} values. The only difference from a full
parametric bca analysis would lie in the nonparametric estimation
of \code{a}, often a negligible error.
}
\examples{
data(diabetes, package = "lars")
Xy <- cbind(diabetes$x, diabetes$y)
rfun <- function(Xy) {
  y <- Xy[, 11]
  X <- Xy[, 1:10]
  summary(lm(y~X) )$adj.r.squared
}
bcajack(x = Xy, B = 1000, func = rfun, m = 40)

}
\references{
DiCiccio T and Efron B (1996). Bootstrap confidence
    intervals. Statistical Science 11, 189-228

Efron B (1987). Better bootstrap confidence
    intervals. JASA 82 171-200
}
